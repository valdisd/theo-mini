# ğŸ› ï¸ Project Overview & Design Notes

This project isn't just a working demo â€” it's a foundation for a scalable and maintainable business context extraction system. I aimed to deliver a clean, focused MVP while anticipating future needs, like expanding to 100+ data points or storing contextual data for analytics.

---

## âœ¨ Core Goals & Guiding Principles

- **Provide a smooth and usable demo**
- **Design for future scalability**
- **Stay close to the task requirements, but not limited by them**
- **Balance structure and flexibility for real-world web content**

---

## ğŸ” Features & Thought Process

### 1. Designed for Scale â€” 100+ Data Points Ready

While the current interface extracts just 3 fields â€” *Mission/Vision*, *Product Description*, and *Unique Value Proposition* â€” the underlying architecture supports many more:

- Data point definitions live in a structured array
- Each field can include type, description, prompt context
- Easy to expand without rewriting routes or UI logic

To support this:

- We **store both raw HTML and extracted text** from the website
- This allows for:
  - Rerunning extractions without re-scraping
  - Generating new fields later from stored content
  - Adding annotations or schema mapping in future

---

### 2. Open vs. Strict Query Modes

Not all company websites are well-written. Some don't state their value clearly. So, I added a toggle for two modes of querying the extracted data:

- ğŸ§± **Strict**: Answers are generated **only from the extracted content**, ensuring tight alignment with scraped data. This suits automated pipelines or high-accuracy use cases.

- ğŸŒ **Open**: Gives the model freedom to use its prior knowledge. Great for exploratory questions, comparisons, or when scraped data is thin.

This flexibility lets users work both programmatically and conversationally â€” depending on their goals.

---

### 3. Smart Scraping Strategy

During testing (e.g. with `selfnamed.com`), I noticed many sites hide useful business info in their **â€œAbout Usâ€** page, not the homepage.

So I added logic to:
- Scan homepage links
- Detect an "about" page if present
- Scrape and combine both pages for better GPT context

This keeps the UX clean while improving extraction accuracy without needing user input.

---

## ğŸ§± Architecture Highlights

### Structure

```
/src
  /app
    /api
      /extract  â†’ Scrapes and extracts content using GPT
      /query    â†’ Handles user follow-up questions
  /components   â†’ Simple UI structure
  /lib
    scraper.ts  â†’ Encapsulates Playwright scraping logic
    prompts.ts  â†’ Centralizes query system prompts
  /config
    constants.ts â†’ Data field config, default modes, etc.
```

### Stack

- **Next.js 15 App Router** â€” clean full-stack separation
- **Playwright** â€” fast, reliable scraping
- **OpenAI GPT-4o** â€” state-of-the-art LLM
- **Tailwind CSS** â€” minimal but effective styling
- **TypeScript** â€” strong typing for safety and scalability
- **pnpm** â€” fast installs and strict dependency control

---

## ğŸ§ª Edge Cases Handled

- Pages with no visible body text
- Invalid or non-HTTP URLs
- GPT errors and token limits
- Sites with content split between homepage and other pages
- Manual retry for OpenAI failures

All are caught and returned with friendly errors to the UI.

---

## ğŸ“¦ Future-Proofing

Hereâ€™s what Iâ€™ve made easier for the next phases:

- âœ… **Database integration** â€” extracted data and raw text are ready to be saved
- âœ… **Reprocessing** â€” no need to scrape again, just re-run extraction
- âœ… **Field expansion** â€” config-driven extraction list
- âœ… **Debugging tools** â€” logs, error messages, and ability to inspect source

---

## ğŸ§  What Iâ€™d Add Next

If this were continued into production, Iâ€™d add:

- Persistent storage (PostgreSQL or SQLite with Prisma)
- Admin dashboard for reviewing & approving extractions
- LLM function-calling support to build structured output
- Field-level confidence scores
- Caching or queueing system for large-scale jobs
- Rate limiting + retry queues

---

## ğŸ’¡ Summary

This challenge was a great opportunity to build something quick, thoughtful, and extendable. I made sure it works well now but doesnâ€™t hit a wall when the requirements grow.

It balances:

- âœ… Task requirements (demo, 3 fields, queries)
- âœ… Realistic complexity of the modern web
- âœ… Flexibility for future enhancements

Thanks for the opportunity! I hope you enjoy using and reviewing it.

â€” Valdis
